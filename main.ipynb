{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78c9ea71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import torch\n",
    "from stock_dataloader import create_stock_dataloader\n",
    "from lstm import StockLSTM\n",
    "from transformer import StockTransformer\n",
    "from model_trainer import train_model, save_model, load_model\n",
    "from evaluation import evaluate_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d17c909a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dataloader\n",
    "\n",
    "# Hyperparameters\n",
    "SEQ_LEN = 100           # default 100; window for set of time-series data points\n",
    "BATCH_SIZE = 32         # default 32; increase if GPU mem allows\n",
    "STOCKS_PER_BUCKET = 5   # default 13; number of stocks per category bucket\n",
    "TRAIN_PER_BUCKET = 3    # default 10; number of training stocks per category bucket\n",
    "\n",
    "stock_csv = 'selected_stocks_data.csv'\n",
    "metadata_csv = 'selected_stocks_quality.csv'\n",
    "stock_dataloader = create_stock_dataloader(stock_csv, metadata_csv, seq_len=SEQ_LEN, batch_size=BATCH_SIZE,\n",
    "                                           stocks_per_bucket=STOCKS_PER_BUCKET, train_per_bucket=TRAIN_PER_BUCKET)\n",
    "train_loader = stock_dataloader['train_loader']\n",
    "eval_loader = stock_dataloader['eval_loader']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96c98d4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create LSTM Model\n",
    "\n",
    "# Hyperparameters\n",
    "INPUT_SIZE = 1          # default 1; based on data\n",
    "HIDDEN_SIZE = 64        # default 64; analogous to D_MODEL; increase to 128 if underfitting\n",
    "NUM_LAYERS = 2          # default 2, re-evaluate if underfitting\n",
    "DROP_OUT = 0.2          # default 0.2; re-evaluate if overfitting\n",
    "\n",
    "lstm_model = StockLSTM(input_size=INPUT_SIZE,\n",
    "                  hidden_size=HIDDEN_SIZE,\n",
    "                  num_layers=NUM_LAYERS,\n",
    "                  dropout=DROP_OUT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60b85af8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Transformer Model\n",
    "\n",
    "# Hyperparameters\n",
    "INP_DIM = 1             # default 1; based on data\n",
    "D_MODEL = 64            # default 64; analogous to HIDDEN_SIZE; re-evaluate if underfitting\n",
    "N_HEADS = 4             # default 4; 64/4 = 16 - standard ratio\n",
    "N_LAYERS = 3            # default 3; re-evaluate if underfitting\n",
    "DIM_FEEDFORWARD = 256   # default 256; 4x D_MODEL is standard\n",
    "DROPOUT = 0.1           # default 0.1; re-evaluate if overfitting\n",
    "OUTPUT_DIM = 1          # default 1; based on data - next-day closing price\n",
    "MAX_LEN = 500           # default 500; should be > SEQ_LEN\n",
    "\n",
    "transformer_model = StockTransformer(inp_dim=INP_DIM,\n",
    "                         d_model=D_MODEL,\n",
    "                         n_heads=N_HEADS,\n",
    "                         n_layers=N_LAYERS,\n",
    "                         dim_feedforward=DIM_FEEDFORWARD,\n",
    "                         dropout=DROPOUT,\n",
    "                         output_dim=OUTPUT_DIM,\n",
    "                         max_len=MAX_LEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dfa1439",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train model\n",
    "\n",
    "# Hyperparameters\n",
    "NUM_EPOCHS = 50         # default 50; increase if underfitting\n",
    "LEARNING_RATE = 0.001   # default 0.001; drop to 3e-4 if unstable\n",
    "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "model_choice = 'Transformer'   # Select 'LSTM' or 'Transformer'\n",
    "model_save_name = f'Stock{model_choice}_ModelMini'\n",
    "model_save = True\n",
    "model_load = True\n",
    "\n",
    "print(f\"Training {model_choice} model on device: {DEVICE}\")\n",
    "if model_choice == 'LSTM':\n",
    "    if model_load:\n",
    "        try:\n",
    "            trained_model = load_model(f'models/{model_save_name}')\n",
    "        except FileNotFoundError as e:\n",
    "            print(e)\n",
    "            trained_model = train_model(lstm_model, train_loader, num_epochs=NUM_EPOCHS, learning_rate=LEARNING_RATE, device=DEVICE)\n",
    "    else:\n",
    "        trained_model = train_model(lstm_model, train_loader, num_epochs=NUM_EPOCHS, learning_rate=LEARNING_RATE, device=DEVICE)\n",
    "    if model_save:\n",
    "        save_model(trained_model, save_name=model_save_name)\n",
    "elif model_choice == 'Transformer':\n",
    "    if model_load:\n",
    "        try:\n",
    "            trained_model = load_model(f'models/{model_save_name}')\n",
    "        except FileNotFoundError as e:\n",
    "            print(e)\n",
    "            trained_model = train_model(transformer_model, train_loader, num_epochs=NUM_EPOCHS, learning_rate=LEARNING_RATE, device=DEVICE)\n",
    "    else:\n",
    "        trained_model = train_model(transformer_model, train_loader, num_epochs=NUM_EPOCHS, learning_rate=LEARNING_RATE, device=DEVICE)\n",
    "    if model_save:\n",
    "        save_model(trained_model, save_name=model_save_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "999d72a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate model\n",
    "evaluate_model(trained_model, eval_loader, device=DEVICE)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
